{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossedAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(1024, 256, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(1024, 256, 1, bias=False)\n",
    "        self.v_conv = nn.Conv1d(1024, 256, 1, bias=False)\n",
    "        self.trans_conv = nn.Conv1d(256, 256, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q_tensor, kv_tensor):\n",
    "        x_q = self.q_conv(q_tensor.permute(0, 2, 1))    \n",
    "        x_k = self.k_conv(kv_tensor.permute(0, 2, 1))  \n",
    "        x_v = self.v_conv(kv_tensor.permute(0, 2, 1))\n",
    "        print('x_q: ', x_q.shape)\n",
    "        energy = torch.matmul(x_q.permute(0, 2, 1), x_k) nn.Softmax\n",
    "        print('energy shape: ', energy.shape)\n",
    "        attention = self.softmax(energy)\n",
    "        # attention = attention / (1e-9 + attention.sum(dim=1, keepdims=True))\n",
    "        x_r = torch.matmul(attention, x_v.permute(0, 2, 1))  \n",
    "        print('x_rs shape: ', x_r.shape)\n",
    "        \n",
    "        res = (x_q - x_r.permute(0, 2, 1))\n",
    "        print('res shape: ', res.shape)\n",
    "        \n",
    "        x_r = self.act(self.after_norm(self.trans_conv(res)))\n",
    "        print('result shape: ', x_r.shape)\n",
    "        \n",
    "        return x_r.permute(0, 2, 1)\n",
    "    \n",
    "ca = CrossedAttention(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_q:  torch.Size([4, 256, 1])\n",
      "energy shape:  torch.Size([4, 1, 6])\n",
      "x_rs shape:  torch.Size([4, 1, 256])\n",
      "res shape:  torch.Size([4, 256, 1])\n",
      "result shape:  torch.Size([4, 256, 1])\n"
     ]
    }
   ],
   "source": [
    "rand_q = torch.ones((4, 1, 1024))\n",
    "rand_kv = torch.ones((4, 6, 1024))\n",
    "\n",
    "result = ca(rand_q, rand_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([4, 1, 1024])\n",
      "x_q:  torch.Size([4, 1024, 1])\n",
      "x_v:  torch.Size([4, 1024, 6])\n",
      "energy:  torch.Size([4, 1, 6])\n",
      "attention:  torch.Size([4, 1, 6])\n",
      "x_r:  torch.Size([4, 1, 1024])\n",
      "res:  torch.Size([4, 1024, 1])\n"
     ]
    }
   ],
   "source": [
    "rand_q = torch.ones((4, 1, 1024))\n",
    "rand_kv = torch.ones((4, 6, 1024))\n",
    "\n",
    "result = ca(rand_q, rand_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1024])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "attention = softmax(energy)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-97f400bc2fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "x_v = x_q\n",
    "x_r = torch.matmul(attention, x_v.permute(0, 2, 1))\n",
    "x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[-0.3655,  0.7285],\n",
      "        [ 0.6512,  2.0699],\n",
      "        [-1.4464,  0.2941]])\n",
      "K\n",
      " tensor([[ 0.2889,  1.9247],\n",
      "        [-1.2538, -1.0563],\n",
      "        [-0.1256,  0.4617]])\n",
      "V\n",
      " tensor([[-0.2521,  1.1885],\n",
      "        [ 0.8997,  0.3598],\n",
      "        [ 1.8922,  0.0591]])\n",
      "Values\n",
      " tensor([[ 0.5572,  0.7237],\n",
      "        [-0.0568,  1.0844],\n",
      "        [ 0.8927,  0.4633]])\n",
      "Attention\n",
      " tensor([[0.5421, 0.1739, 0.2840],\n",
      "        [0.9063, 0.0057, 0.0880],\n",
      "        [0.2113, 0.5506, 0.2381]])\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "seq_len, d_k = 3, 2\n",
    "# pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossedAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(1024, 256, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(1024, 256, 1, bias=False)\n",
    "        self.v_conv = nn.Conv1d(1024, 1024, 1, bias=False)\n",
    "        self.trans_conv = nn.Conv1d(1024, 1024, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(1024)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q_tensor, kv_tensor):\n",
    "        print('q_tensor: ', q_tensor.shape)\n",
    "        x_q = self.q_conv(q_tensor.permute(0, 2, 1))    \n",
    "        x_k = self.k_conv(kv_tensor.permute(0, 2, 1))  \n",
    "        x_v = self.v_conv(kv_tensor.permute(0, 2, 1))\n",
    "        print('x_q: ', x_q.shape)\n",
    "        energy = torch.matmul(x_q.permute(0, 2, 1), x_k)\n",
    "        print('energy shape: ', energy.shape)\n",
    "        attention = self.softmax(energy)\n",
    "        # attention = attention / (1e-9 + attention.sum(dim=1, keepdims=True))\n",
    "        x_r = torch.matmul(attention, x_v.permute(0, 2, 1))  \n",
    "        print('x_r shape: ', x_r.shape)\n",
    "        \n",
    "        res = (q_tensor - x_r).permute(0, 2, 1)\n",
    "        print('res shape: ', res.shape)\n",
    "        \n",
    "        x_r = self.act(self.after_norm(self.trans_conv(res)))\n",
    "        x_r = x_r.permute(0, 2, 1) + q_tensor\n",
    "        print('result shape: ', x_r.shape)\n",
    "        \n",
    "        return x_r\n",
    "    \n",
    "cb = CrossedAttention(1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_tensor:  torch.Size([4, 1, 1024])\n",
      "x_q:  torch.Size([4, 256, 1])\n",
      "energy shape:  torch.Size([4, 1, 6])\n",
      "x_r shape:  torch.Size([4, 1, 1024])\n",
      "res shape:  torch.Size([4, 1024, 1])\n",
      "result shape:  torch.Size([4, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "rand_q = torch.ones((4, 1, 1024))\n",
    "rand_kv = torch.ones((4, 6, 1024))\n",
    "\n",
    "result = cb(rand_q, rand_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
