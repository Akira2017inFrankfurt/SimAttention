{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SA_Layer(nn.Module):\n",
    "    # todo: add mutil-head attention\n",
    "    # todo: keep this class, design new class with name: SA_MH_Layer\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(channels, channels, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(channels, channels, 1, bias=False)\n",
    "        self.v_conv = nn.Conv1d(channels, channels, 1, bias=False)\n",
    "        \n",
    "        self.trans_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        bs, f, p = x.shape\n",
    "        print('bs, f, p: ', bs, f, p, '\\n')\n",
    "        \n",
    "        x_q = self.q_conv(x)  # b, n, c\n",
    "        print('x_q: ', x_q.shape)\n",
    "        x_q = x_q.reshape(bs, 4, -1, p).permute(0, 1, 3, 2)\n",
    "        print('x_q: ', x_q.shape, '\\n')\n",
    "        \n",
    "        x_k = self.k_conv(y)\n",
    "        print('x_k: ', x_k.shape)\n",
    "        x_k = x_k.reshape(bs, 4, -1, y.shape[-1])\n",
    "        print('x_k: ', x_k.shape, '\\n')\n",
    "        \n",
    "        xy = torch.matmul(x_q, x_k)\n",
    "        print('xy: ', xy.shape, '\\n')\n",
    "        \n",
    "        x_v = self.v_conv(z)\n",
    "        print('x_v: ', x_v.shape)\n",
    "        x_v = x_v.reshape(bs, 4, -1, z.shape[-1]).permute(0, 1, 3, 2)\n",
    "        print('x_v: ', x_v.shape, '\\n')\n",
    "        \n",
    "        xyz = torch.matmul(xy, x_v)\n",
    "        print('1st time get xyz: ', xyz.shape)\n",
    "        xyz = xyz.permute(0, 1, 3, 2).reshape(bs, p, -1)\n",
    "        print('after permute xyz: ', xyz.shape)\n",
    "        \n",
    "        xyz = self.trans_conv(xyz)\n",
    "        print('after trans_conv xyz: ', xyz.shape)\n",
    "        \n",
    "        xyz = self.act(self.after_norm(xyz - x))\n",
    "        print('after relu xyz: ', xyz.shape)\n",
    "        \n",
    "        xyz = x + xyz\n",
    "        print('after residual xyz: ', xyz.shape)\n",
    "        return xyz\n",
    "    \n",
    "ca = SA_Layer(256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs, f, p:  4 256 256 \n",
      "\n",
      "x_q:  torch.Size([4, 256, 256])\n",
      "x_q:  torch.Size([4, 4, 256, 64]) \n",
      "\n",
      "x_k:  torch.Size([4, 256, 256])\n",
      "x_k:  torch.Size([4, 4, 64, 256]) \n",
      "\n",
      "xy:  torch.Size([4, 4, 256, 256]) \n",
      "\n",
      "x_v:  torch.Size([4, 256, 256])\n",
      "x_v:  torch.Size([4, 4, 256, 64]) \n",
      "\n",
      "1st time get xyz:  torch.Size([4, 4, 256, 64])\n",
      "after permute xyz:  torch.Size([4, 256, 256])\n",
      "after trans_conv xyz:  torch.Size([4, 256, 256])\n",
      "after relu xyz:  torch.Size([4, 256, 256])\n",
      "after residual xyz:  torch.Size([4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((4, 256, 256))\n",
    "y = torch.ones((4, 256, 256))\n",
    "z = torch.ones((4, 256, 256))\n",
    "\n",
    "result = ca(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一点需要确认的就是现在这个输入的维度，每一维分别代表什么；\n",
    "注意，我们需要确保，第二个维度就是feature维度；\n",
    "\n",
    "根据pct的encoder来看，是直接对输入进来的x进行conv，所以原来的维度应该就是对的；\n",
    "那么现在就是进行替换了。\n",
    "好了，基本也就是这些，将下面这个类拷贝进入encoder脚本中。\n",
    "把用到SA Layer的四个地方完全替换，然后测试看能不能顺利运行。\n",
    "测试通过；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SA_M_Layer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(channels, channels, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(channels, channels, 1, bias=False)\n",
    "        self.v_conv = nn.Conv1d(channels, channels, 1, bias=False)\n",
    "        \n",
    "        self.trans_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, f, p = x.shape\n",
    "        x_q = self.q_conv(x)  # b, n, c\n",
    "        x_q = x_q.reshape(bs, 4, -1, p).permute(0, 1, 3, 2)\n",
    "        x_k = self.k_conv(x)\n",
    "        x_k = x_k.reshape(bs, 4, -1, x.shape[-1])\n",
    "        xy = torch.matmul(x_q, x_k)\n",
    "        x_v = self.v_conv(x)\n",
    "        x_v = x_v.reshape(bs, 4, -1, x.shape[-1]).permute(0, 1, 3, 2)\n",
    "        xyz = torch.matmul(xy, x_v)\n",
    "        xyz = xyz.permute(0, 1, 3, 2).reshape(bs, p, -1)\n",
    "        xyz = self.trans_conv(xyz)\n",
    "        xyz = self.act(self.after_norm(xyz - x))\n",
    "        xyz = x + xyz\n",
    "        return xyz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
